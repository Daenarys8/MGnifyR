% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MGnifyR.R
\name{mgnify_retrieve_json}
\alias{mgnify_retrieve_json}
\title{Low level MGnify API handler}
\usage{
mgnify_retrieve_json(client, path = "biomes", complete_url = NULL,
  qopts = NULL, maxhits = 200, usecache = F, Debug = F)
}
\arguments{
\item{client}{MGnifyR client}

\item{path}{top level search point for the query. One of \code{biomes}, \code{samples}, \code{runs} etc. Basically includes
all parts of the URL between the base API url and the parameter specifications}

\item{complete_url}{\emph{complete} url to search, usuaally retrieved from previous query in the "related" section.}

\item{qopts}{named list or vector containing options/filters to be URL encoded and appended to query as key/value pairs}

\item{maxhits}{Maxmium number of data entries to return. The actual number of hits returned may be higher than this value,
as this parameter only clamps after each full page is processed. Set to <=0 to disable - i.e. retrieve all items.}

\item{usecache}{Should successful queries be cached on disk locally? There are unresolved questions about whether this is
a sensible thing to do, but it remains as an option. It probably makes sense for single accession grabs, but not for
(filtered) queries - which are liable to change as new data is added to MGnify. Also caching only works for the first page.}

\item{Debug}{Should we print out lots of information while doing the grabbing?}
}
\value{
\code{list} of results after pagination is dealt with.
}
\description{
\code{mgnify_retrieve_json} deals with handles the actual HTTP GET calls for the MGnifyR package, handling API pagination,
local result caching, and  authentication cookies for access
to restricted or pre-release datasets.Although principally intended for internal MGnifyR use , it's exported for direct invocation.
Generally though it's not recommended for use by users.
}
